{"cells":[{"cell_type":"markdown","metadata":{"id":"-IDJduvwMd_u"},"source":["## Fine-tuning"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Zjbt-_pYNAO0"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-13 18:58:15.609205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","/Users/eddie/miniconda3/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Generating train split: 1447 examples [00:00, 56240.17 examples/s]\n","Map: 100%|██████████| 1447/1447 [00:01<00:00, 1164.59 examples/s]\n","  0%|          | 2/1810 [00:50<12:57:21, 25.80s/it]"]}],"source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n","from transformers import DataCollatorForLanguageModeling\n","from datasets import load_dataset\n","\n","\n","def fine_tune_gpt2(model_name, train_file, output_dir):\n","    # Load GPT-2 model and tokenizer\n","    model = GPT2LMHeadModel.from_pretrained(model_name)\n","    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    # Load training dataset\n","    dataset = load_dataset(\"text\", data_files={'train': train_file})\n","    def encode(examples):\n","        return tokenizer(examples['text'], truncation=True, max_length=128)\n","    dataset = dataset.map(encode, batched=True)\n","    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n","\n","    # Create data collator for language modeling\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=False)\n","\n","    # Set training arguments\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        overwrite_output_dir=True,\n","        num_train_epochs=5,\n","        per_device_train_batch_size=4,\n","        save_steps=10_000,\n","        save_total_limit=2,\n","        logging_dir='./logs'  # Add logging\n","    )\n","\n","    # Train the model\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator,\n","        train_dataset=dataset['train']\n","    )\n","\n","    trainer.train()\n","\n","    # Save the fine-tuned model\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","\n","# Usage\n","fine_tune_gpt2(\"gpt2\", \"dialogues.txt\", \"pretrained\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMQjOaX5BV93Zu8WCjzYG9u","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
