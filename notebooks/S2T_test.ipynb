{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34472,"status":"ok","timestamp":1713447803889,"user":{"displayName":"Eddie Nevander hellström","userId":"16169287474616511517"},"user_tz":-120},"id":"M0pu1qIKaB6x","outputId":"b3efb51f-eff3-4f28-e5a0-2a9d1f3bb434"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#FOR COLAB:\n","# import os\n","# from google.colab import drive\n","\n","# # Mount drive and change directory\n","# drive.mount('/content/drive')\n","\n","# path = 'NLP Project'\n","# os.chdir(f'/content/drive/MyDrive/{path}')\n","# agent_file = 'gridspace-stanford-harper-valley/data/audio/agent/0a6f57765f4641a2.wav'\n","# caller_file = 'gridspace-stanford-harper-valley/data/audio/caller/0a6f57765f4641a2.wav'"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/nicco/NLP/project/notebooks\n"]}],"source":["#LOCAL:\n","import os \n","#get current directory\n","path = os.getcwd()\n","#change to parent directory\n","os.chdir(os.path.dirname(path))\n","print(path)\n","agent_file = 'gridspace_repo/data/audio/agent/0a6f57765f4641a2.wav'\n","caller_file = 'gridspace_repo/data/audio/caller/0a6f57765f4641a2.wav'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28269,"status":"ok","timestamp":1713451181023,"user":{"displayName":"Eddie Nevander hellström","userId":"16169287474616511517"},"user_tz":-120},"id":"rlqkwppRcoq-","outputId":"51270f4b-54e9-42c0-de97-9e538e1024e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: SpeechRecognition in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (3.10.3)\n","Requirement already satisfied: requests>=2.26.0 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from SpeechRecognition) (2.31.0)\n","Requirement already satisfied: typing-extensions in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from SpeechRecognition) (4.11.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n","Requirement already satisfied: pydub in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (0.25.1)\n"]}],"source":["# Install needed packages\n","!pip install SpeechRecognition\n","!pip install pydub"]},{"cell_type":"markdown","metadata":{"id":"xDPu1r-RhC1Q"},"source":["## Transcribe the audio files"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pydub in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (0.25.1)\n","Requirement already satisfied: speechrecognition in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (3.10.3)\n","Requirement already satisfied: requests>=2.26.0 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from speechrecognition) (2.31.0)\n","Requirement already satisfied: typing-extensions in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from speechrecognition) (4.11.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (2024.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (2.2.1)\n","Collecting numpy\n","  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Installing collected packages: numpy\n","Successfully installed numpy-1.26.4\n","Collecting matplotlib\n","  Using cached matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","Collecting pyparsing>=2.3.1\n","  Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n","Collecting cycler>=0.10\n","  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Collecting fonttools>=4.22.0\n","  Using cached fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","Collecting kiwisolver>=1.3.1\n","  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n","Collecting pillow>=8\n","  Using cached pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n","Collecting contourpy>=1.0.1\n","  Using cached contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n","Requirement already satisfied: numpy>=1.21 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from matplotlib) (24.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /home/nicco/NLP/nlpvenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n","Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4 pillow-10.3.0 pyparsing-3.1.2\n","\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install pydub\n","!pip install speechrecognition\n","!pip install numpy\n","!pip install matplotlib\n","!pip install itertools"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"fPEHnQhqtIM6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/nicco/NLP/nlpvenv/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n","  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"]}],"source":["from pydub import AudioSegment, silence\n","import speech_recognition as sr\n","import numpy as np\n","from itertools import zip_longest"]},{"cell_type":"markdown","metadata":{},"source":["## Split in terms of silence pauses"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":419,"status":"ok","timestamp":1713452691929,"user":{"displayName":"Eddie Nevander hellström","userId":"16169287474616511517"},"user_tz":-120},"id":"brwnseratJ9n"},"outputs":[],"source":["def split_audio_on_silence(audio_path, min_silence_len=2500):\n","    \"\"\"Load an audio file and split on silences.\"\"\"\n","    audio = AudioSegment.from_wav(audio_path)\n","    # This may need adjustment based on the specific audio file characteristics\n","    return silence.split_on_silence(audio, min_silence_len=min_silence_len, silence_thresh=audio.dBFS-15, keep_silence=100)\n","\n","def transcribe_segments(segments):\n","    \"\"\"Transcribe a list of AudioSegment objects.\"\"\"\n","    recognizer = sr.Recognizer()\n","    transcriptions = []\n","\n","    for i, segment in enumerate(segments):\n","        # Export segment to a temporary WAV file\n","        segment.export(f\"temp_segment{i}.wav\", format=\"wav\")\n","\n","        # Recognize the audio\n","        with sr.AudioFile(\"temp_segment.wav\") as source:\n","            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n","            audio_data = recognizer.record(source)\n","            try:\n","                transcription = recognizer.recognize_google(audio_data, language='en-US', show_all=False)\n","                transcriptions.append(transcription)\n","            except sr.UnknownValueError:\n","                print(f\"Segment {i+1}: Google Speech Recognition could not understand audio\")\n","            except sr.RequestError as e:\n","                print(f\"Segment {i+1}: Could not request results; {e}\")\n","\n","    return transcriptions\n","\n","def main(audio_file):\n","    segments = split_audio_on_silence(audio_file)\n","    print(segments)\n","    transcriptions = transcribe_segments(segments)\n","    return transcriptions"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20564,"status":"ok","timestamp":1713452714389,"user":{"displayName":"Eddie Nevander hellström","userId":"16169287474616511517"},"user_tz":-120},"id":"X4t7Ye6Vkwpl","outputId":"a9499d09-a5dd-4a6c-ce1a-d1b3811641cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[<pydub.audio_segment.AudioSegment object at 0x7fb222c9eb30>, <pydub.audio_segment.AudioSegment object at 0x7fb2201a9bd0>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee645330>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee645c90>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee645300>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee645210>]\n","Segment 1: Google Speech Recognition could not understand audio\n","Segment 2: Google Speech Recognition could not understand audio\n","Segment 3: Google Speech Recognition could not understand audio\n","Segment 4: Google Speech Recognition could not understand audio\n","Segment 5: Google Speech Recognition could not understand audio\n","Segment 6: Google Speech Recognition could not understand audio\n","[<pydub.audio_segment.AudioSegment object at 0x7fb1ee645210>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee644ac0>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee644880>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee644a90>, <pydub.audio_segment.AudioSegment object at 0x7fb1ee645d50>]\n","Segment 1: Google Speech Recognition could not understand audio\n","Segment 2: Google Speech Recognition could not understand audio\n","Segment 3: Google Speech Recognition could not understand audio\n","Segment 4: Google Speech Recognition could not understand audio\n","Segment 5: Google Speech Recognition could not understand audio\n","Conversation\n","\n"]}],"source":["# Paths to files\n","agent_transcription = main(agent_file)\n","caller_transcription = main(caller_file)\n","\n","print('Conversation\\n')\n","for agent, caller in zip_longest(agent_transcription, caller_transcription, fillvalue=\"???\"):\n","    print(\"count \\r\")\n","    if agent:\n","        print(f'Agent: {agent}')\n","    if caller:\n","        print(f'Caller: {caller}')"]},{"cell_type":"markdown","metadata":{},"source":["## Split following the transcripts (just caller for now)"]},{"cell_type":"markdown","metadata":{},"source":["have to look at the readme file to understand which fields are crucial"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Google Speech Recognition could not understand audio from segment 0\n","Google Speech Recognition could not understand audio from segment 3\n","Google Speech Recognition could not understand audio from segment 5\n","Google Speech Recognition could not understand audio from segment 6\n","Google Speech Recognition could not understand audio from segment 8\n","Google Speech Recognition could not understand audio from segment 9\n","Google Speech Recognition could not understand audio from segment 10\n","Google Speech Recognition could not understand audio from segment 11\n","Google Speech Recognition could not understand audio from segment 12\n","Google Speech Recognition could not understand audio from segment 13\n","Google Speech Recognition could not understand audio from segment 15\n","Google Speech Recognition could not understand audio from segment 16\n","Google Speech Recognition could not understand audio from segment 17\n","Google Speech Recognition could not understand audio from segment 18\n","Google Speech Recognition could not understand audio from segment 19\n","Segment 0:\n","Machine transcript: did not understand:(\n","Human transcript: hello this is harper valley national bank my name is megan how can i help you today\n","Similarity: 0.1553398058252427\n","Segment 1:\n","Machine transcript: hi my name is Patricia\n","Human transcript: [noise] hi my name is patricia jones\n","Similarity: 0.7241379310344828\n","Segment 2:\n","Machine transcript: I would like to transfer money between my account\n","Human transcript: i would like to transfer money between my accounts\n","Similarity: 0.9696969696969697\n","Segment 3:\n","Machine transcript: did not understand:(\n","Human transcript: what is the transfer amount\n","Similarity: 0.2127659574468085\n","Segment 4:\n","Machine transcript: the mount is 102\n","Human transcript: the amount is a hundred and two dollars\n","Similarity: 0.4727272727272727\n","Segment 5:\n","Machine transcript: did not understand:(\n","Human transcript: [noise]\n","Similarity: 0.2222222222222222\n","Segment 6:\n","Machine transcript: did not understand:(\n","Human transcript: and what is the source account\n","Similarity: 0.12\n","Segment 7:\n","Machine transcript: from my checking account to my savings account\n","Human transcript: from my checking account to my savings account\n","Similarity: 1.0\n","Segment 8:\n","Machine transcript: did not understand:(\n","Human transcript: [noise]\n","Similarity: 0.2222222222222222\n","Segment 9:\n","Machine transcript: did not understand:(\n","Human transcript: [noise]\n","Similarity: 0.2222222222222222\n","Segment 10:\n","Machine transcript: did not understand:(\n","Human transcript: [noise]\n","Similarity: 0.2222222222222222\n","Segment 11:\n","Machine transcript: did not understand:(\n","Human transcript: a hundred and two dollars has been transferred from your checking account to your savings account\n","Similarity: 0.13675213675213677\n","Segment 12:\n","Machine transcript: did not understand:(\n","Human transcript: is there anything else i can help you with\n","Similarity: 0.22580645161290322\n","Segment 13:\n","Machine transcript: did not understand:(\n","Human transcript: thank you\n","Similarity: 0.20689655172413793\n","Segment 14:\n","Machine transcript: no that was all thank you\n","Human transcript: no that was all thank you\n","Similarity: 1.0\n","Segment 15:\n","Machine transcript: did not understand:(\n","Human transcript: [noise]\n","Similarity: 0.2222222222222222\n","Segment 16:\n","Machine transcript: did not understand:(\n","Human transcript: okay thank you for calling have a great day\n","Similarity: 0.19047619047619047\n","Segment 17:\n","Machine transcript: did not understand:(\n","Human transcript: bye\n","Similarity: 0.08695652173913043\n","Segment 18:\n","Machine transcript: did not understand:(\n","Human transcript: bye\n","Similarity: 0.08695652173913043\n","Segment 19:\n","Machine transcript: did not understand:(\n","Human transcript: [noise]\n","Similarity: 0.2222222222222222\n"]}],"source":["import json\n","import speech_recognition as sr\n","from pydub import AudioSegment\n","from difflib import SequenceMatcher\n","\n","def load_transcript(file_path):\n","    with open(file_path, 'r') as f:\n","        transcript = json.load(f)\n","    return transcript\n","\n","def split_audio(file_path, transcript):\n","    audio = AudioSegment.from_wav(file_path)\n","    segments = []\n","    for segment in transcript:\n","        start = segment['start_ms'] #when the segment start (offset from the beginning of the audio file in milliseconds)\n","        duration = segment['duration_ms'] #how long the segment is (in milliseconds)\n","        segments.append(audio[start:start+duration]) \n","    return segments\n","\n","def transcribe_segments(segments):\n","    r = sr.Recognizer()\n","    transcriptions = []\n","    for i, segment in enumerate(segments):\n","        #create a temp folder to store the audio files\n","        os.makedirs(\"temp\", exist_ok=True)\n","        segment.export(f\"temp/temp_segment{i}.wav\", format=\"wav\")\n","        with sr.AudioFile(f\"temp/temp_segment{i}.wav\") as source:\n","            audio = r.record(source)\n","            try:\n","                transcription = r.recognize_google(audio)\n","                transcriptions.append(transcription)\n","            except sr.UnknownValueError: # Google Speech Recognition could not understand the audio, catch this exception\n","                transcriptions.append(\"did not understand:(\")\n","                print(f\"Google Speech Recognition could not understand audio from segment {i}\")\n","    return transcriptions\n","\n","def compare_transcriptions(transcriptions, transcript):\n","    for i, transcription in enumerate(transcriptions):\n","        human_transcript = transcript[i]['human_transcript']\n","        similarity = SequenceMatcher(None, transcription, human_transcript).ratio()\n","        print(f\"Segment {i}:\")\n","        print(f\"Machine transcript: {transcription}\")\n","        print(f\"Human transcript: {human_transcript}\")\n","        print(f\"Similarity: {similarity}\")\n","\n","def main(audio_file_path, transcript_file_path):\n","    transcript = load_transcript(transcript_file_path)\n","    segments = split_audio(audio_file_path, transcript)\n","    transcriptions = transcribe_segments(segments)\n","    compare_transcriptions(transcriptions, transcript)\n","\n","#\n","main('gridspace_repo/data/audio/caller/0a6f57765f4641a2.wav', 'gridspace_repo/data/transcript/0a6f57765f4641a2.json')"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["#delete the temp folder\n","import shutil\n","shutil.rmtree(\"temp\")"]},{"cell_type":"markdown","metadata":{},"source":["About 70% of the segments cannot be understood by this model, we will have to fine tune a new one."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMJbY1R0N16MspY2uyXJpWW","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
